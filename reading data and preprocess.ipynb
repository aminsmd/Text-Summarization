{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cat,n):\n",
    "    sent = []\n",
    "    for i in range(1,int(n)+1):\n",
    "        if(i<10):\n",
    "            ad = \"00\" + str(i)\n",
    "        elif(i<100):\n",
    "            ad = \"0\" + str(i)\n",
    "        elif(cat==\"sport\" and i==199): #error openning the file\n",
    "            continue\n",
    "        else:\n",
    "            ad = str(i)\n",
    "        f = open(\"BBC News Summary/News Articles/\"+ cat +\"/\"+ad+\".txt\")\n",
    "        n = \"\"\n",
    "        for line in f: \n",
    "            n += line\n",
    "        sent.append(n)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(cat_sent):\n",
    "    a = []\n",
    "    b = []\n",
    "    for i in cat_sent:\n",
    "        a = []\n",
    "        for j in sent_tokenize(i):\n",
    "            a.append(j)\n",
    "        b.append(a)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading data, tokenizing and saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = []\n",
    "sports = read_data(\"sport\",511)\n",
    "t = tokenizer(sports)\n",
    "f = open(\"BBC News Summary/News Articles/sport/dumps/sport_tokenized_sentences\",\"wb\")\n",
    "pk.dump(t,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business = []\n",
    "business = read_data(\"business\",510)\n",
    "t = tokenizer(business)\n",
    "f = open(\"BBC News Summary/News Articles/business/dumps/business_tokenized_sentences\",\"wb\")\n",
    "pk.dump(t,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment = []\n",
    "entertainment = read_data(\"entertainment\",386)\n",
    "t = tokenizer(entertainment)\n",
    "f = open(\"BBC News Summary/News Articles/entertainment/dumps/entertainment_tokenized_sentences\",\"wb\")\n",
    "pk.dump(t,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = []\n",
    "politics = read_data(\"politics\",417)\n",
    "t = tokenizer(politics)\n",
    "f = open(\"BBC News Summary/News Articles/politics/dumps/politics_tokenized_sentences\",\"wb\")\n",
    "pk.dump(t,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = []\n",
    "tech = read_data(\"tech\",401)\n",
    "t = tokenizer(tech)\n",
    "f = open(\"BBC News Summary/News Articles/tech/dumps/tech_tokenized_sentences\",\"wb\")\n",
    "pk.dump(t,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word to vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_size = 300\n",
    "i = 0\n",
    "with open('cc.en.300.vec') as f:\n",
    "    for line in f:\n",
    "        if i > 400000:\n",
    "            break\n",
    "        i += 1\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = [float(i) for i in values[1:]]\n",
    "        embeddings_index[word] = coefs\n",
    "embeddings_index['<PAD>'] = [0] * 300\n",
    "embeddings_index['<UNK>'] = [1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"farsi embeddings\",\"wb\") as f:\n",
    "    pk.dump(embeddings_index , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
